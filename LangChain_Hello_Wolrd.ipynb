{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkU9OKJZkcSoNuKbc3naPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoursdream123/PIAIC-Q2-PROJECTS/blob/main/LangChain_Hello_Wolrd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 1: LangChain Hello World project**"
      ],
      "metadata": {
        "id": "OEAo6DASOrz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For creating a simple LangChain Colab Notebook that uses the Google Gemini Flash 1.5 model to answer user questions. This example below is provided to help you get started assumes you have access to the Gemini API and a basic Python environment."
      ],
      "metadata": {
        "id": "hYiZp4P1PXho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Install Required Libraries**\n",
        "\n",
        "Run the following commands to ensure all required libraries are installed:"
      ],
      "metadata": {
        "id": "ySY829VKQLkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "5CHUndg4JEDj",
        "outputId": "c751234f-927a-4d65-defc-a77d32090fcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai"
      ],
      "metadata": {
        "id": "icOeHOOgJTqa",
        "outputId": "2c73bd96-57ce-4130-d695-e756ccaafae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.10.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Import Libraries**\n",
        "\n",
        "Update your imports to include the correct modules for Google Gemini:"
      ],
      "metadata": {
        "id": "OHICQZUrQsb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import google.generativeai as genai\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any"
      ],
      "metadata": {
        "id": "-be3ifJiJre4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Set Up the Gemini API**\n",
        "\n",
        "Obtain your API key from the Google AI Studio or Google Cloud, then configure the Gemini model:"
      ],
      "metadata": {
        "id": "OQ0L2AcVQ8y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY_1')"
      ],
      "metadata": {
        "id": "sxj4cOIYJwry",
        "outputId": "8bde4cf4-05c3-438c-9b47-d706f8178d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyAQzDApXh0p0iUpFsxuc0dDj5T3LBLTCy4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Retrieve the API key securely\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY_1')\n",
        "\n",
        "# Verify if the key is loaded\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"API key not found. Make sure 'GOOGLE_API_KEY_1' is set in userdata.\")\n",
        "\n",
        "# Initialize the Gemini API\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "Dn-8oeoEJ4DM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Create a Custom Wrapper for Google Gemini**\n",
        "\n",
        "LangChain allows creating a custom LLM class. Here’s the wrapper for Gemini:"
      ],
      "metadata": {
        "id": "gNE-yZDGRHQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiLLM(LLM):\n",
        "    model: str = \"gemini-1.5-flash\"  # Model name\n",
        "    temperature: float = 0.7\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"google_gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        model = genai.GenerativeModel(self.model)\n",
        "        response = model.generate_content(prompt, generation_config={\"temperature\": self.temperature})\n",
        "        return response.text\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model, \"temperature\": self.temperature}"
      ],
      "metadata": {
        "id": "VeDZepGZKNO-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Integrate with LangChain**\n",
        "\n",
        "Now you can use the GeminiLLM class as a replacement for GoogleGeminiFlash:"
      ],
      "metadata": {
        "id": "-BvIZqqbRQLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini LLM\n",
        "llm = GeminiLLM(model=\"gemini-1.5-flash\", temperature=0.7)\n",
        "\n",
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Create the LangChain pipeline\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "hcnY1FyGKYF4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Ask a sample question\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "id": "7Z_ZOOIMLv2G",
        "outputId": "4e492f0d-c1cc-461c-b3e2-49dd9f0b686c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a framework for developing applications powered by language models.  It simplifies the process of building applications that utilize LLMs by providing modular components and tools for:\n",
            "\n",
            "* **Connecting to various LLMs:**  LangChain allows you to easily switch between different language models (like OpenAI, Hugging Face Hub models, etc.) without needing to rewrite significant portions of your code.\n",
            "\n",
            "* **Managing memory:**  It offers mechanisms to allow LLMs to \"remember\" previous interactions within a conversation, improving context and coherence.  This is particularly important for longer conversations or complex tasks.\n",
            "\n",
            "* **Chain creation:** LangChain enables you to chain together multiple calls to LLMs or other components (like databases or APIs) to perform more sophisticated tasks.  This allows for building complex workflows that leverage the strengths of LLMs while integrating them with other data sources.\n",
            "\n",
            "* **Agents:**  It provides tools for building agents that can decide which LLMs or tools to use based on the task at hand.  This allows for more autonomous and adaptive applications.\n",
            "\n",
            "In essence, LangChain acts as a powerful toolkit that streamlines the development of LLM-powered applications, making them easier to build, more robust, and more adaptable to various use cases.  It handles much of the boilerplate code, allowing developers to focus on the application logic and integration.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Steps in Colab Project**"
      ],
      "metadata": {
        "id": "rSijj0_mRcO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Experiment with Prompts**\n",
        "\n",
        "You can create multiple prompt templates to see how the model responds to various formats and tasks."
      ],
      "metadata": {
        "id": "Pq8zE5K2Rq1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template for answering factual questions\n",
        "fact_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a knowledgeable assistant. Answer this factual question:\\n{question}\"\n",
        ")\n",
        "\n",
        "# Template for creative storytelling\n",
        "story_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"You are a creative storyteller. Write a short story about the following topic:\\n{topic}\"\n",
        ")\n",
        "\n",
        "# Using a different template\n",
        "question = \"Tell me a short story about a quaid e azam.\"\n",
        "response = LLMChain(llm=llm, prompt=story_template).run({\"topic\": question})\n",
        "print(\"Story Response:\", response)\n"
      ],
      "metadata": {
        "id": "66nehmN7MF0W",
        "outputId": "0ca75185-d621-4be8-c4e2-130fc48407bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story Response: The old man sat on the veranda, the setting sun painting the Lahore sky in hues of apricot and rose.  His hands, gnarled like the ancient banyan tree in his garden, trembled slightly as he held a chipped teacup.  He wasn't the Quaid-e-Azam, not anymore.  He was just Muhammad Ali Jinnah, a man wrestling with memories.\n",
            "\n",
            "The whispers of the past swirled around him, louder than the evening’s chirping crickets. He saw himself, young and sharp, a lawyer battling for justice in the Bombay High Court, his voice ringing with an unwavering conviction.  He saw the burgeoning movement, the fervent hope, the escalating tensions – the seeds of a nation sown in the fertile ground of struggle.\n",
            "\n",
            "He remembered the debates, the endless negotiations, the crushing weight of responsibility.  The faces of his colleagues, some supportive, some skeptical, some outright hostile, flickered before his tired eyes.  He felt again the icy grip of doubt, the gnawing fear of failure, the loneliness of leadership.  The burden of a nation's destiny rested heavily on his shoulders, even now, decades later.\n",
            "\n",
            "A single tear traced a path down his weathered cheek.  It wasn't a tear of sorrow, not exactly.  It was a tear of profound understanding, a silent acknowledgment of the sacrifices made, the battles fought, the compromises endured.  He had seen the birth of Pakistan, a nation forged in the fires of division and aspiration.  He had tasted the sweet victory, but also the bitter aftertaste of its imperfections.\n",
            "\n",
            "He sipped his tea, the warmth spreading through him like a gentle embrace.  He wasn't just remembering the Quaid-e-Azam, the iconic figure, the statesman.  He was remembering the man behind the legend – the father, the friend, the man who carried the weight of a nation's dreams on his frail shoulders, a man who, in the end, simply longed for peace.  The sun dipped below the horizon, leaving behind a sky filled with the quiet promise of a new dawn, a dawn he would not see, but one he had helped to create.  And in the stillness of the evening, he found a measure of solace.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Add Memory (Multi-Turn Conversations)**\n",
        "\n",
        "LangChain provides memory for maintaining context across multiple interactions."
      ],
      "metadata": {
        "id": "ycAx1TH1R81t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reuire module installation\n",
        "!pip install langchain-experimental -q"
      ],
      "metadata": {
        "id": "XRLuFcDXMVPu",
        "outputId": "202b68b3-1a1a-454a-88dc-3b4ae76abb36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a conversational chain with memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Start a conversation\n",
        "response1 = conversation.run(\"What is LangChain?\")\n",
        "print(\"Q1:\", response1)\n",
        "\n",
        "response2 = conversation.run(\"Can you explain it in simple words?\")\n",
        "print(\"Q2:\", response2)"
      ],
      "metadata": {
        "id": "no_CQP8yMc_Z",
        "outputId": "99ce8db7-9f2c-4cdf-a3b2-997d9add569f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-0daba4cfee94>:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-32-0daba4cfee94>:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with various components that you can assemble to create sophisticated and useful LLM-based applications.  It's not an LLM itself, but rather a way to *use* LLMs more effectively.\n",
            "\n",
            "Specifically, LangChain provides several key features:\n",
            "\n",
            "* **Modules for connecting to different LLMs:**  This means you can easily swap between different providers like OpenAI, Hugging Face Hub, or even your own self-hosted models.  You don't need to rewrite your code each time you want to experiment with a different LLM.\n",
            "\n",
            "* **Chains:** This is a core concept in LangChain.  Chains allow you to sequence multiple calls to LLMs or other utilities.  For example, you might have a chain that first summarizes a document, then answers a question based on that summary. This allows for more complex workflows than simply making a single LLM call.  There are various types of chains, including sequential chains, map-reduce chains, and more.\n",
            "\n",
            "* **Indexes:**  LangChain offers tools for indexing your own data.  This is crucial if you want your LLM application to work with specific documents or datasets, rather than just relying on the LLM's general knowledge.  You can index documents in various formats (like PDFs, text files, etc.) and then query the LLM about the indexed information.  Different indexing methods are available, offering trade-offs between speed and accuracy.\n",
            "\n",
            "* **Agents:**  Agents allow your application to decide which tools to use to answer a user's question.  For example, an agent might decide to use a search engine to gather information before querying an LLM.  This lets you build applications that are more resourceful and capable of handling complex tasks.  LangChain offers different agent architectures, each with its own strengths and weaknesses.\n",
            "\n",
            "* **Memory:**  LangChain provides mechanisms for LLMs to \"remember\" previous interactions within a conversation.  This is essential for creating conversational AI applications where context is important.  Different memory types are available, allowing you to control how much context is retained and how it's managed.\n",
            "\n",
            "\n",
            "In short, LangChain simplifies the process of building applications that leverage the power of LLMs by providing a structured and modular approach.  It handles the complexities of interacting with different LLMs, managing data, and orchestrating complex workflows, allowing developers to focus on the application logic.\n",
            "\n",
            "Q2: Imagine you have LEGOs, but instead of building houses, you're building AI apps that use super-smart language models (like ChatGPT).  LangChain is the instruction manual and a bunch of special LEGO pieces that make it easier to build those AI apps.\n",
            "\n",
            "It lets you:\n",
            "\n",
            "* **Use different smart language models easily:**  Swap between different brands of super-smart LEGOs without rebuilding everything.\n",
            "* **Combine steps:** Build complex things by connecting multiple steps together, like first summarizing a book and then answering questions about it.\n",
            "* **Use your own information:**  Add your own LEGOs (your own documents) to the build.\n",
            "* **Make smart decisions:**  Build AI that knows when to use a search engine (like asking Google for help) before answering your question.\n",
            "* **Remember things:**  Make your AI remember what you talked about earlier in the conversation.\n",
            "\n",
            "So, LangChain is a toolkit to make building AI apps much simpler and more powerful.  It's not the smart language model itself, but the tool that helps you *use* the smart language model to build amazing things.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "id": "Y1soK1WOMkb_",
        "outputId": "618b3b62-8366-43f6-ff63-826e012dd88c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content='LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with various components that you can assemble to create sophisticated and useful LLM-based applications.  It\\'s not an LLM itself, but rather a way to *use* LLMs more effectively.\\n\\nSpecifically, LangChain provides several key features:\\n\\n* **Modules for connecting to different LLMs:**  This means you can easily swap between different providers like OpenAI, Hugging Face Hub, or even your own self-hosted models.  You don\\'t need to rewrite your code each time you want to experiment with a different LLM.\\n\\n* **Chains:** This is a core concept in LangChain.  Chains allow you to sequence multiple calls to LLMs or other utilities.  For example, you might have a chain that first summarizes a document, then answers a question based on that summary. This allows for more complex workflows than simply making a single LLM call.  There are various types of chains, including sequential chains, map-reduce chains, and more.\\n\\n* **Indexes:**  LangChain offers tools for indexing your own data.  This is crucial if you want your LLM application to work with specific documents or datasets, rather than just relying on the LLM\\'s general knowledge.  You can index documents in various formats (like PDFs, text files, etc.) and then query the LLM about the indexed information.  Different indexing methods are available, offering trade-offs between speed and accuracy.\\n\\n* **Agents:**  Agents allow your application to decide which tools to use to answer a user\\'s question.  For example, an agent might decide to use a search engine to gather information before querying an LLM.  This lets you build applications that are more resourceful and capable of handling complex tasks.  LangChain offers different agent architectures, each with its own strengths and weaknesses.\\n\\n* **Memory:**  LangChain provides mechanisms for LLMs to \"remember\" previous interactions within a conversation.  This is essential for creating conversational AI applications where context is important.  Different memory types are available, allowing you to control how much context is retained and how it\\'s managed.\\n\\n\\nIn short, LangChain simplifies the process of building applications that leverage the power of LLMs by providing a structured and modular approach.  It handles the complexities of interacting with different LLMs, managing data, and orchestrating complex workflows, allowing developers to focus on the application logic.\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you explain it in simple words?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Imagine you have LEGOs, but instead of building houses, you're building AI apps that use super-smart language models (like ChatGPT).  LangChain is the instruction manual and a bunch of special LEGO pieces that make it easier to build those AI apps.\\n\\nIt lets you:\\n\\n* **Use different smart language models easily:**  Swap between different brands of super-smart LEGOs without rebuilding everything.\\n* **Combine steps:** Build complex things by connecting multiple steps together, like first summarizing a book and then answering questions about it.\\n* **Use your own information:**  Add your own LEGOs (your own documents) to the build.\\n* **Make smart decisions:**  Build AI that knows when to use a search engine (like asking Google for help) before answering your question.\\n* **Remember things:**  Make your AI remember what you talked about earlier in the conversation.\\n\\nSo, LangChain is a toolkit to make building AI apps much simpler and more powerful.  It's not the smart language model itself, but the tool that helps you *use* the smart language model to build amazing things.\\n\", additional_kwargs={}, response_metadata={})]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Explore Gemini Features**\n",
        "\n",
        "Fine-tune Gemini responses by adjusting parameters like temperature and max_tokens.\n",
        "\n",
        "Temperature: Controls creativity (higher = more creative). Max Tokens: Limits response length."
      ],
      "metadata": {
        "id": "jMUZVY3fSJUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust model settings\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY_1\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")"
      ],
      "metadata": {
        "id": "aPYO8nOlMmym"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Combining It All**\n",
        "\n",
        "Combine multiple enhancements—like memory, new prompts, and tools—into a full pipeline"
      ],
      "metadata": {
        "id": "c9weCieZSTdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# configure model\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY_1\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")\n",
        "# Template\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer this question:\\n{question}\"\n",
        ")\n",
        "\n",
        "# Memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Chain with memory\n",
        "conversation = LLMChain(llm=llm, prompt=template, memory=memory)"
      ],
      "metadata": {
        "id": "4ADtqHiTN863"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run interactions\n",
        "response1 = conversation.run({\"question\": \"define science?\"})\n",
        "print(response1)"
      ],
      "metadata": {
        "id": "uaSqhX-6OOeN",
        "outputId": "9c074f26-a1f2-4960-d26e-b49c6b952005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Science is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.  It's a process of observation, experimentation, and reasoning used to understand the natural world.  Key characteristics include:\n",
            "\n",
            "* **Empirical:** Based on observation and experimentation, not just speculation or belief.\n",
            "* **Testable:**  Scientific explanations must be able to be tested through observation or experimentation.  This involves formulating hypotheses that can be proven or disproven.\n",
            "* **Repeatable:**  Experiments and observations should be repeatable by others to verify results.\n",
            "* **Falsifiable:**  A scientific theory must be capable of being proven wrong.  If a theory cannot be disproven, it's not considered scientific.\n",
            "* **Objective:**  Scientists strive to minimize bias and subjective interpretations in their work.\n",
            "* **Cumulative:**  Scientific knowledge builds upon previous discoveries and research.\n",
            "\n",
            "In short, science is a way of knowing about the world, characterized by its reliance on evidence and a commitment to rigorous testing.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = conversation.run({\"question\": \"Explain it in a funny way.\"})\n",
        "print(response2)"
      ],
      "metadata": {
        "id": "u2HIxyUaOUGb",
        "outputId": "13b0547d-e286-4774-e3e5-71c6f5f177d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide me with the question you'd like me to explain in a funny way!  I'm ready to unleash my inner comedic genius (or at least attempt to).  Just give me the question, and I'll do my best to make you chuckle.  Think of me as the stand-up comedian of explanations – expect some dad jokes and possibly a misplaced prop.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "id": "-ucEjl_wSgH-",
        "outputId": "108f03d0-8fcc-4527-a3d6-20b2e019cbfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='define science?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Science is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.  It's a process of observation, experimentation, and reasoning used to understand the natural world.  Key characteristics include:\\n\\n* **Empirical:** Based on observation and experimentation, not just speculation or belief.\\n* **Testable:**  Scientific explanations must be able to be tested through observation or experimentation.  This involves formulating hypotheses that can be proven or disproven.\\n* **Repeatable:**  Experiments and observations should be repeatable by others to verify results.\\n* **Falsifiable:**  A scientific theory must be capable of being proven wrong.  If a theory cannot be disproven, it's not considered scientific.\\n* **Objective:**  Scientists strive to minimize bias and subjective interpretations in their work.\\n* **Cumulative:**  Scientific knowledge builds upon previous discoveries and research.\\n\\nIn short, science is a way of knowing about the world, characterized by its reliance on evidence and a commitment to rigorous testing.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain it in a funny way.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Please provide me with the question you'd like me to explain in a funny way!  I'm ready to unleash my inner comedic genius (or at least attempt to).  Just give me the question, and I'll do my best to make you chuckle.  Think of me as the stand-up comedian of explanations – expect some dad jokes and possibly a misplaced prop.\\n\", additional_kwargs={}, response_metadata={})]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}